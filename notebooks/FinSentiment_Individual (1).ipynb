{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2bWJ-8wRQe_"
      },
      "source": [
        "#  FinSentiment 2.1: Refactorizaci贸n Quality-of-Life\n",
        "\n",
        "Versi贸n mejorada con feedback de usuario para reducir ruido legal y mejorar la visualizaci贸n.\n",
        "\n",
        "### Mejoras v2.1:\n",
        "1. **Filtro Legal**: Se eliminan oraciones 'disclaimer' (Forward-Looking Statements) para purificar el sentimiento.\n",
        "2. **Stopwords Extendidas**: Limpieza de palabras financieras gen茅ricas ('may', 'company', 'results') en nubes de palabras.\n",
        "3. **Score Normalizado**: Visualizaci贸n de **Z-Score** (desviaci贸n vs media) para ver cambios relativos, no absolutos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u68FgDrQRQfM"
      },
      "outputs": [],
      "source": [
        "# @title 1. Instalaci贸n de Dependencias\n",
        "!pip install sec-edgar-downloader transformers torch pandas numpy matplotlib seaborn beautifulsoup4 yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBIz8flxRQfP"
      },
      "outputs": [],
      "source": [
        "# @title 2. Definici贸n del Motor (Core Engine v2.1)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Configurar estilos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# --- MDULO 1: INGESTA ---\n",
        "class SECLoader:\n",
        "    def __init__(self, data_dir=\"data\", email=\"research@example.com\", company=\"Personal Research\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
        "        self.downloader = Downloader(company, email, os.path.join(data_dir, \"raw\"))\n",
        "\n",
        "    def download_filings(self, ticker: str, amount: int = 2):\n",
        "        path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        if os.path.exists(path):\n",
        "             print(f\"[>] Archivos para {ticker} ya descargados.\")\n",
        "             return\n",
        "\n",
        "        print(f\"[>] Descargando {amount} reportes para {ticker}...\")\n",
        "        try:\n",
        "            self.downloader.get(\"10-K\", ticker, limit=amount)\n",
        "            print(\"[+] Descarga completa.\")\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error en descarga: {e}\")\n",
        "\n",
        "    def extract_date(self, content: str) -> str:\n",
        "        patterns = [\n",
        "            r'FILED AS OF DATE:\\s+(\\d{8})',\n",
        "            r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})'\n",
        "        ]\n",
        "        for p in patterns:\n",
        "            match = re.search(p, content)\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "                return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
        "        return None\n",
        "\n",
        "    def extract_mda(self, html_content: str) -> str:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        patterns = [r'Item\\s+7\\.\\s+Management', r\"Management's\\s+Discussion\", r'Item\\s+7\\.']\n",
        "        start_idx = -1\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text, re.IGNORECASE)\n",
        "            if match: start_idx = match.start(); break\n",
        "        if start_idx == -1: return text[:50000]\n",
        "        return text[start_idx:start_idx+30000]\n",
        "\n",
        "    def process_filings(self, ticker: str):\n",
        "        raw_path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        processed_data = []\n",
        "        for root, _, files in os.walk(raw_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".txt\") and \"primary\" not in file:\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                            content = f.read()\n",
        "                        mda = self.extract_mda(content)\n",
        "                        date = self.extract_date(content)\n",
        "                        if len(mda) > 500:\n",
        "                            processed_data.append({'text': mda, 'date': date, 'accession': file})\n",
        "                    except: pass\n",
        "        return processed_data\n",
        "\n",
        "# --- MDULO 2 PREPROCESAMIENTO (CON FILTRO LEGAL) ---\n",
        "class TextPreprocessor:\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return \"\".join(ch for ch in text if ch.isprintable()).strip()\n",
        "\n",
        "    def is_legal_noise(self, sentence: str) -> bool:\n",
        "        # Palabras clave de disclaimers y riesgo legal\n",
        "        legal_keywords = [\n",
        "            'forward-looking', 'safe harbor', 'uncertainty', 'may differ',\n",
        "            'subject to error', 'actual results', 'factors that could cause',\n",
        "            'statements regarding', 'cautionary note', 'risk factors',\n",
        "            'include but are not limited to', 'assumptions'\n",
        "        ]\n",
        "        s_lower = sentence.lower()\n",
        "        return any(kw in s_lower for kw in legal_keywords)\n",
        "\n",
        "    def split_sentences(self, text):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "        valid_sentences = []\n",
        "        for s in sentences:\n",
        "            s = s.strip()\n",
        "            # Filtro 1: Longitud\n",
        "            if len(s) > 20 and len(s.split()) >= 4:\n",
        "                # Filtro 2: Ruido Legal (Falso Negativo Commmon Source)\n",
        "                if not self.is_legal_noise(s):\n",
        "                    valid_sentences.append(s)\n",
        "        return valid_sentences\n",
        "\n",
        "# --- MDULO 3: MODELO FINBERT ---\n",
        "class FinBertModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"[*] Cargando FinBERT en {self.device}...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(self.device)\n",
        "        self.labels = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        if not sentences: return pd.DataFrame()\n",
        "        batch_size = 32\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            for j, s in enumerate(batch):\n",
        "                idx = np.argmax(probs[j])\n",
        "                results.append({\n",
        "                    \"sentence\": s,\n",
        "                    \"sentiment\": self.labels[idx],\n",
        "                    \"pos_val\": probs[j][0],\n",
        "                    \"neg_val\": probs[j][1]\n",
        "                })\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "# --- MDULO 4: DATA Y ANALYTICS ---\n",
        "def get_market_data(ticker, start_date, end_date):\n",
        "    print(f\"[*] Descargando precios de {ticker}...\")\n",
        "    df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "    return df['Close']\n",
        "\n",
        "def analyze_keywords(sentences, top_k=10):\n",
        "    # Stopwords expandidas (Legal + Financiero Gen茅rico)\n",
        "    stopwords = set([\n",
        "        'the', 'and', 'of', 'to', 'in', 'a', 'that', 'for', 'is', 'on', 'with', 'as',\n",
        "        'our', 'we', 'are', 'by', 'it', 'from', 'an', 'be', 'files', 'company',\n",
        "        # Solicitadas por el usuario:\n",
        "        'may', 'such', 'period', 'results', 'year', 'quarter', 'other', 'have', 'million', 'billion'\n",
        "    ])\n",
        "    words = []\n",
        "    for s in sentences:\n",
        "        tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', s)\n",
        "        words.extend([w.lower() for w in tokens if w.lower() not in stopwords])\n",
        "    return Counter(words).most_common(top_k)\n",
        "\n",
        "def show_dashboard(ticker, results_df, market_data=None):\n",
        "    print(\"\\n\" + \"=\"*60 + f\"\\n SCOREBOARD: {ticker}\\n\" + \"=\"*60)\n",
        "\n",
        "    # 1. Normalizaci贸n (Z-Score)\n",
        "    summary = results_df.groupby('date')[['pos_val', 'neg_val']].mean()\n",
        "    summary['net_score'] = summary['pos_val'] - summary['neg_val']\n",
        "\n",
        "    # Calcular Z-Score para ver cambios relativos\n",
        "    mean_score = summary['net_score'].mean()\n",
        "    std_score = summary['net_score'].std()\n",
        "    if std_score == 0: std_score = 1 # Evitar Div0\n",
        "    summary['z_score'] = (summary['net_score'] - mean_score) / std_score\n",
        "\n",
        "    # Mostrar tabla coloreada por Z-Score (lo importante es si sube o baja vs media)\n",
        "    print(\"[M茅trica Clave] Z-Score: Desviaci贸n Est谩ndar respecto a la media hist贸rica.\")\n",
        "    display(summary.style.background_gradient(cmap='RdYlGn', subset=['z_score']))\n",
        "\n",
        "    # 2. Visualizaci贸n Precio vs Sentimiento Normalizado\n",
        "    if market_data is not None:\n",
        "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "        color = '#2E86AB'\n",
        "        ax1.set_xlabel('Fecha')\n",
        "        ax1.set_ylabel('Sentiment Z-Score (Normalizado)', color=color, fontweight='bold')\n",
        "        # Graficar Z-Score en lugar de raw score\n",
        "        ax1.plot(pd.to_datetime(summary.index), summary['z_score'], 'o-', color=color, linewidth=2)\n",
        "        ax1.axhline(0, color='gray', linestyle='--', alpha=0.5, label='Media Hist贸rica')\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = '#F24236'\n",
        "        ax2.set_ylabel(f'Precio {ticker} ($)', color=color, fontweight='bold')\n",
        "        ax2.plot(market_data.index, market_data, color=color, alpha=0.6, linewidth=1.5)\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        plt.title(f\"{ticker}: Precio vs Sentimiento Relativo (Z-Score)\", fontsize=14)\n",
        "        plt.show()\n",
        "\n",
        "    # 3. Deep Dive (ltimo reporte)\n",
        "    latest_date = results_df['date'].max()\n",
        "    latest_df = results_df[results_df['date'] == latest_date]\n",
        "\n",
        "    print(f\"\\n>>> DEEP DIVE (Reporte {latest_date})\")\n",
        "    print(\"\\n[TOP 5 POSITIVAS]\")\n",
        "    for _, r in latest_df.nlargest(5, 'pos_val').iterrows():\n",
        "        print(f\"(+) {r['pos_val']:.2f}: {r['sentence'][:150]}...\")\n",
        "\n",
        "    print(\"\\n[TOP 5 NEGATIVAS - FILTRADAS]\")\n",
        "    for _, r in latest_df.nlargest(5, 'neg_val').iterrows():\n",
        "        print(f\"(-) {r['neg_val']:.2f}: {r['sentence'][:150]}...\")\n",
        "\n",
        "    pos_text = latest_df[latest_df['sentiment']=='positive']['sentence']\n",
        "    neg_text = latest_df[latest_df['sentiment']=='negative']['sentence']\n",
        "    print(f\"\\n[KEYWORDS FRECUENTES] (Sin stopwords financieras)\")\n",
        "    print(f\"Positivas: {[k for k,v in analyze_keywords(pos_text)]}\")\n",
        "    print(f\"Negativas: {[k for k,v in analyze_keywords(neg_text)]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7_h0ECGRQfV"
      },
      "outputs": [],
      "source": [
        "# @title 3. EJECUCIN MAESTRA\n",
        "TICKER = \"ADBE\" # @param {type:\"string\"}\n",
        "NUM_REPORTS = 3 # @param {type:\"integer\"}\n",
        "\n",
        "# 1. Ingesta\n",
        "loader = SECLoader()\n",
        "loader.download_filings(TICKER, NUM_REPORTS)\n",
        "raw_docs = loader.process_filings(TICKER)\n",
        "\n",
        "if not raw_docs:\n",
        "    print(\"[!] No se encontraron datos.\")\n",
        "else:\n",
        "    # 2. An谩lisis Robustos\n",
        "    model = FinBertModel()\n",
        "    prep = TextPreprocessor()\n",
        "\n",
        "    all_results = []\n",
        "    print(\"\\n[*] Iniciando an谩lisis de sentimeinto...\")\n",
        "    for doc in raw_docs:\n",
        "        # Ahora split_sentences aplica el FILTRO LEGAL autom谩ticamente\n",
        "        sentences = prep.split_sentences(prep.clean_text(doc['text']))\n",
        "        print(f\"Oraciones v谩lidas en {doc.get('date')}: {len(sentences)}\")\n",
        "\n",
        "        df_sent = model.predict(sentences)\n",
        "        if not df_sent.empty:\n",
        "            df_sent['date'] = doc.get('date', '2023-01-01')\n",
        "            all_results.append(df_sent)\n",
        "\n",
        "    # 3. Dashboard + Market Data\n",
        "    if all_results:\n",
        "        final_df = pd.concat(all_results)\n",
        "        dates = pd.to_datetime(final_df['date']).sort_values()\n",
        "        start = dates.min() - timedelta(days=30)\n",
        "        end = dates.max() + timedelta(days=30)\n",
        "\n",
        "        try:\n",
        "            prices = get_market_data(TICKER, start, end)\n",
        "            show_dashboard(TICKER, final_df, prices)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Fall贸 descarga de precios: {e}\")\n",
        "            show_dashboard(TICKER, final_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}