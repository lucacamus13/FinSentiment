{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 游 FinSentiment 3.0: Sector Screener (Batch Edition)\n",
        "\n",
        "Versi칩n escalable para analizar m칰ltiples empresas del sector tecnol칩gico de forma secuencial y comparativa.\n",
        "\n",
        "### Nuevas Caracter칤sticas (Fase 7):\n",
        "1. **Batch Processing**: An치lisis autom치tico de m칰ltiples tickers definidos por el usuario.\n",
        "2. **Robustez**: Manejo de errores encapsulado (si falla uno, sigue el siguiente).\n",
        "3. **Screener Visual**: Ranking de sentimiento (Z-Score) para comparar empresas.\n",
        "4. **Core v2.1**: Filtros legales y normalizaci칩n estad칤stica ya integrados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. Instalaci칩n de Dependencias\n",
        "!pip install sec-edgar-downloader transformers torch pandas numpy matplotlib seaborn beautifulsoup4 yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. Definici칩n del Motor (Core Engine v2.1)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Configurar estilos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# --- M칍DULO 1: INGESTA ---\n",
        "class SECLoader:\n",
        "    def __init__(self, data_dir=\"data\", email=\"research@example.com\", company=\"Personal Research\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
        "        self.downloader = Downloader(company, email, os.path.join(data_dir, \"raw\"))\n",
        "\n",
        "    def download_filings(self, ticker: str, amount: int = 1):\n",
        "        path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        # En batch mode, a veces conviene forzar descarga si amount es bajo\n",
        "        print(f\"[>] ({ticker}) Iniciando descarga de {amount} reporte(s)...\")\n",
        "        try:\n",
        "            self.downloader.get(\"10-K\", ticker, limit=amount)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error en descarga {ticker}: {e}\")\n",
        "\n",
        "    def extract_date(self, content: str) -> str:\n",
        "        patterns = [\n",
        "            r'FILED AS OF DATE:\\s+(\\d{8})',\n",
        "            r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})'\n",
        "        ]\n",
        "        for p in patterns:\n",
        "            match = re.search(p, content)\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "                return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
        "        return None\n",
        "\n",
        "    def extract_mda(self, html_content: str) -> str:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        patterns = [r'Item\\s+7\\.\\s+Management', r\"Management's\\s+Discussion\", r'Item\\s+7\\.']\n",
        "        start_idx = -1\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text, re.IGNORECASE)\n",
        "            if match: start_idx = match.start(); break\n",
        "        if start_idx == -1: return text[:50000]\n",
        "        return text[start_idx:start_idx+30000]\n",
        "\n",
        "    def process_filings(self, ticker: str):\n",
        "        raw_path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        processed_data = []\n",
        "        for root, _, files in os.walk(raw_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".txt\") and \"primary\" not in file:\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                            content = f.read()\n",
        "                        mda = self.extract_mda(content)\n",
        "                        date = self.extract_date(content)\n",
        "                        if len(mda) > 500:\n",
        "                            processed_data.append({'text': mda, 'date': date, 'accession': file})\n",
        "                    except: pass\n",
        "        # Ordenar por fecha reciente\n",
        "        return sorted(processed_data, key=lambda x: x.get('date', '1900'), reverse=True)\n",
        "\n",
        "# --- M칍DULO 2 NOISE FILTER ---\n",
        "class TextPreprocessor:\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return \"\".join(ch for ch in text if ch.isprintable()).strip()\n",
        "    \n",
        "    def is_legal_noise(self, sentence: str) -> bool:\n",
        "        legal_keywords = [\n",
        "            'forward-looking', 'safe harbor', 'uncertainty', 'may differ', \n",
        "            'subject to error', 'actual results', 'factors that could cause',\n",
        "            'statements regarding', 'cautionary note', 'risk factors', \n",
        "            'include but are not limited to', 'assumptions'\n",
        "        ]\n",
        "        return any(kw in sentence.lower() for kw in legal_keywords)\n",
        "\n",
        "    def split_sentences(self, text):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "        valid_sentences = []\n",
        "        for s in sentences:\n",
        "            s = s.strip()\n",
        "            if len(s) > 20 and len(s.split()) >= 4:\n",
        "                if not self.is_legal_noise(s):\n",
        "                    valid_sentences.append(s)\n",
        "        return valid_sentences\n",
        "\n",
        "# --- M칍DULO 3 FINBERT ---\n",
        "class FinBertModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"[*] Cargando FinBERT en {self.device}...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(self.device)\n",
        "        self.labels = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        if not sentences: return pd.DataFrame()\n",
        "        batch_size = 32\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            for j, s in enumerate(batch):\n",
        "                results.append({\n",
        "                    \"sentence\": s,\n",
        "                    \"pos_val\": probs[j][0],\n",
        "                    \"neg_val\": probs[j][1]\n",
        "                })\n",
        "        return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. Ejecuci칩n Batch con Selector de Tickers\n",
        "\n",
        "# Inicializar Modelos\n",
        "print(\"[*] Inicializando Motor NLP...\")\n",
        "loader = SECLoader()\n",
        "model = FinBertModel()\n",
        "prep = TextPreprocessor()\n",
        "\n",
        "# --- CONFIGURACI칍N DE USUARIO ---\n",
        "TICKERS_INPUT = \"META, AAPL, MSFT, GOOGL, AMZN\" # @param {type:\"string\"}\n",
        "NUM_REPORTS = 1 # @param {type:\"integer\"}\n",
        "\n",
        "# Parsear tickers (separados por coma)\n",
        "TICKERS = [t.strip().upper() for t in TICKERS_INPUT.split(',') if t.strip()]\n",
        "print(f\"\\n游닇 TARGETS: {TICKERS}\")\n",
        "print(f\"游늼 REPORTES POR EMPRESA: {NUM_REPORTS}\")\n",
        "\n",
        "def analyze_ticker(ticker_symbol, amount=1):\n",
        "    \"\"\"Funci칩n Maestra: Procesa una empresa de principio a fin.\"\"\"\n",
        "    print(f\"\\n\" + \"-\"*50)\n",
        "    print(f\" 游끽 PROCESANDO AGENTE: {ticker_symbol}\")\n",
        "    print(\"-\"*50)\n",
        "    try:\n",
        "        # 1. Download\n",
        "        loader.download_filings(ticker_symbol, amount=amount)\n",
        "        \n",
        "        # 2. Process\n",
        "        docs = loader.process_filings(ticker_symbol)\n",
        "        if not docs:\n",
        "            print(f\"[!] No se encontraron reportes legibles para {ticker_symbol}\")\n",
        "            return None\n",
        "            \n",
        "        # Tomamos el/los reportes solicitados (hasta donde haya)\n",
        "        reports_to_scan = docs[:amount]\n",
        "        print(f\"[>] Analizando {len(reports_to_scan)} documento(s)...\")\n",
        "        \n",
        "        # Para el screener simple, usamos el M츼S RECIENTE, \n",
        "        # pero podriamos promediar si el usuario pide m치s.\n",
        "        # Aqu칤 tomaremos el 칰ltimo (칤ndice 0 tras el sort) para el 'current sentiment'.\n",
        "        latest_doc = reports_to_scan[0]\n",
        "        report_date = latest_doc.get('date', 'Unknown')\n",
        "        print(f\"[>] Analizando reporte principal del {report_date}...\")\n",
        "        \n",
        "        # 3. Clean & Predict\n",
        "        sentences = prep.split_sentences(prep.clean_text(latest_doc['text']))\n",
        "        print(f\"[i] Oraciones extra칤das: {len(sentences)}\")\n",
        "        \n",
        "        df = model.predict(sentences)\n",
        "        \n",
        "        if df.empty:\n",
        "            return None\n",
        "            \n",
        "        # 4. Calculate Metrics\n",
        "        net_score = df['pos_val'].mean() - df['neg_val'].mean()\n",
        "        print(f\"[ok] Net Score Calculado: {net_score:.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'ticker': ticker_symbol,\n",
        "            'date': report_date,\n",
        "            'net_score': net_score\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Fallo cr칤tico en {ticker_symbol}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- BUCLE PRINCIPAL ---\n",
        "sector_results = []\n",
        "\n",
        "for t in TICKERS:\n",
        "    res = analyze_ticker(t, amount=NUM_REPORTS)\n",
        "    if res:\n",
        "        sector_results.append(res)\n",
        "\n",
        "# Construir DataFrame\n",
        "df_sector = pd.DataFrame(sector_results)\n",
        "\n",
        "if not df_sector.empty:\n",
        "    # Calcular Z-Score relativo al GRUPO (Sectorial)\n",
        "    sector_mean = df_sector['net_score'].mean()\n",
        "    sector_std = df_sector['net_score'].std()\n",
        "    if sector_std == 0: sector_std = 1\n",
        "    \n",
        "    df_sector['z_score'] = (df_sector['net_score'] - sector_mean) / sector_std\n",
        "\n",
        "    # --- MARKET DATA INTEGRATION (ALPHA HUNTER) ---\n",
        "    print(\"\\n[*] Descargando datos de mercado (6 meses)...\")\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=180)\n",
        "    \n",
        "    # Descarga optimizada en batch\n",
        "    tickers_list = df_sector['ticker'].tolist()\n",
        "    try:\n",
        "        market_data = yf.download(tickers_list, start=start_date, end=end_date)['Close']\n",
        "        \n",
        "        # Calcular Retorno 6M (%)\n",
        "        if len(tickers_list) == 1:\n",
        "            # pandas Series handling\n",
        "            price_return = (market_data.iloc[-1] - market_data.iloc[0]) / market_data.iloc[0] * 100\n",
        "            df_sector['price_return_6m'] = price_return\n",
        "        else:\n",
        "            # pandas DataFrame handling\n",
        "            returns = (market_data.iloc[-1] - market_data.iloc[0]) / market_data.iloc[0] * 100\n",
        "            # Mapear los retornos al dataframe (indice de returns es ticker)\n",
        "            df_sector['price_return_6m'] = df_sector['ticker'].map(returns)\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"[!] Error calculando retornos: {e}\")\n",
        "        df_sector['price_return_6m'] = 0.0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" SECTOR SCREENER RESULTS (Alpha)\")\n",
        "    print(\"=\"*40)\n",
        "    display(df_sector.sort_values('z_score', ascending=False))\n",
        "    \n",
        "    # --- VISUALIZACI칍N 1: BARRAS Z-SCORE ---\n",
        "    plt.figure(figsize=(10, max(6, len(df_sector)*0.6)))\n",
        "    plot_data = df_sector.sort_values('z_score', ascending=False)\n",
        "    colors = ['#2ecc71' if x >= 0 else '#e74c3c' for x in plot_data['z_score']]\n",
        "    sns.barplot(x='z_score', y='ticker', data=plot_data, palette=colors)\n",
        "    plt.title('Sentiment Leaderboard (Z-Score)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Z-Score')\n",
        "    plt.show()\n",
        "    \n",
        "    # --- VISUALIZACI칍N 2: ALPHA SCATTER PLOT ---\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    sns.scatterplot(\n",
        "        data=df_sector, \n",
        "        x='z_score', \n",
        "        y='price_return_6m', \n",
        "        s=150, \n",
        "        color='#2E86AB',\n",
        "        edgecolor='black',\n",
        "        alpha=0.8\n",
        "    )\n",
        "    \n",
        "    # Etiquetas (Labels)\n",
        "    for i in range(df_sector.shape[0]):\n",
        "        plt.text(\n",
        "            x=df_sector.z_score.iloc[i]+0.02, \n",
        "            y=df_sector.price_return_6m.iloc[i]+0.5, \n",
        "            s=df_sector.ticker.iloc[i], \n",
        "            fontweight='bold',\n",
        "            fontsize=10\n",
        "        )\n",
        "\n",
        "    # Cuadrantes (Alpha Cross)\n",
        "    plt.axvline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    \n",
        "    plt.title('Alpha Hunter: IA Sentiment vs Market Reality (6M)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Sentiment Z-Score (IA)', fontsize=12)\n",
        "    plt.ylabel('6-Month Price Return (%)', fontsize=12)\n",
        "    \n",
        "    # Labels Cuadrantes (Din치mico seg칰n rango)\n",
        "    # Usamos max/min para posicionar texto en las esquinas\n",
        "    xlims = plt.xlim()\n",
        "    ylims = plt.ylim()\n",
        "    \n",
        "    # Top Right\n",
        "    plt.text(xlims[1], ylims[1], 'WINNERS\\n(Price Up + Sentiment Up)', ha='right', va='top', color='green', alpha=0.5)\n",
        "    # Top Left\n",
        "    plt.text(xlims[0], ylims[1], 'SKEPTICISM?\\n(Price Up + Sentiment Down)', ha='left', va='top', color='orange', alpha=0.5)\n",
        "    # Bottom Right\n",
        "    plt.text(xlims[1], ylims[0], 'OPPORTUNITY?\\n(Price Down + Sentiment Up)', ha='right', va='bottom', color='blue', alpha=0.5)\n",
        "    # Bottom Left\n",
        "    plt.text(xlims[0], ylims[0], 'LOSERS\\n(Price Down + Sentiment Down)', ha='left', va='bottom', color='red', alpha=0.5)\n",
        "    \n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[!] No se obtuvieron resultados v치lidos para generar el gr치fico.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}