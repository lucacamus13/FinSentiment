{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  FinSentiment 3.0: Sector Screener (Batch Edition)\n",
        "\n",
        "Versi贸n escalable para analizar m煤ltiples empresas del sector tecnol贸gico de forma secuencial y comparativa.\n",
        "\n",
        "### Nuevas Caracter铆sticas (Fase 7 - 9):\n",
        "1. **Batch Processing**: An谩lisis autom谩tico de m煤ltiples tickers definidos por el usuario.\n",
        "2. **Robustez**: Manejo de errores encapsulado (si falla uno, sigue el siguiente).\n",
        "3. **Screener Visual**: Ranking de sentimiento (Z-Score) para comparar empresas.\n",
        "4. **Alpha Hunter**: Gr谩fico de dispersi贸n Sentimiento IA vs Retorno de Mercado (Sincronizado).\n",
        "5. **Macro Heatmap**: Evoluci贸n hist贸rica del sentimiento sectorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. Instalaci贸n de Dependencias\n",
        "!pip install sec-edgar-downloader transformers torch pandas numpy matplotlib seaborn beautifulsoup4 yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. Definici贸n del Motor (Core Engine v2.1)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Configurar estilos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# --- MDULO 1: INGESTA ---\n",
        "class SECLoader:\n",
        "    def __init__(self, data_dir=\"data\", email=\"research@example.com\", company=\"Personal Research\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
        "        self.downloader = Downloader(company, email, os.path.join(data_dir, \"raw\"))\n",
        "\n",
        "    def download_filings(self, ticker: str, amount: int = 1):\n",
        "        path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        print(f\"[>] ({ticker}) Iniciando descarga de hasta {amount} reportes...\")\n",
        "        try:\n",
        "            self.downloader.get(\"10-K\", ticker, limit=amount)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error en descarga {ticker}: {e}\")\n",
        "\n",
        "    def extract_date(self, content: str) -> str:\n",
        "        patterns = [\n",
        "            r'FILED AS OF DATE:\\s+(\\d{8})',\n",
        "            r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})'\n",
        "        ]\n",
        "        for p in patterns:\n",
        "            match = re.search(p, content)\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "                return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
        "        return None\n",
        "\n",
        "    def extract_mda(self, html_content: str) -> str:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        patterns = [r'Item\\s+7\\.\\s+Management', r\"Management's\\s+Discussion\", r'Item\\s+7\\.']\n",
        "        start_idx = -1\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text, re.IGNORECASE)\n",
        "            if match: start_idx = match.start(); break\n",
        "        if start_idx == -1: return text[:50000]\n",
        "        return text[start_idx:start_idx+30000]\n",
        "\n",
        "    def process_filings(self, ticker: str):\n",
        "        raw_path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        processed_data = []\n",
        "        for root, _, files in os.walk(raw_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(\".txt\") and \"primary\" not in file:\n",
        "                    try:\n",
        "                        with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                            content = f.read()\n",
        "                        mda = self.extract_mda(content)\n",
        "                        date = self.extract_date(content)\n",
        "                        if len(mda) > 500:\n",
        "                            processed_data.append({'text': mda, 'date': date, 'accession': file})\n",
        "                    except: pass\n",
        "        # Ordenar por fecha reciente\n",
        "        return sorted(processed_data, key=lambda x: x.get('date', '1900'), reverse=True)\n",
        "\n",
        "# --- MDULO 2 NOISE FILTER ---\n",
        "class TextPreprocessor:\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return \"\".join(ch for ch in text if ch.isprintable()).strip()\n",
        "    \n",
        "    def is_legal_noise(self, sentence: str) -> bool:\n",
        "        legal_keywords = [\n",
        "            'forward-looking', 'safe harbor', 'uncertainty', 'may differ', \n",
        "            'subject to error', 'actual results', 'factors that could cause',\n",
        "            'statements regarding', 'cautionary note', 'risk factors', \n",
        "            'include but are not limited to', 'assumptions'\n",
        "        ]\n",
        "        return any(kw in sentence.lower() for kw in legal_keywords)\n",
        "\n",
        "    def split_sentences(self, text):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "        valid_sentences = []\n",
        "        for s in sentences:\n",
        "            s = s.strip()\n",
        "            if len(s) > 20 and len(s.split()) >= 4:\n",
        "                if not self.is_legal_noise(s):\n",
        "                    valid_sentences.append(s)\n",
        "        return valid_sentences\n",
        "\n",
        "# --- MDULO 3 FINBERT ---\n",
        "class FinBertModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"[*] Cargando FinBERT en {self.device}...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(self.device)\n",
        "        self.labels = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        if not sentences: return pd.DataFrame()\n",
        "        batch_size = 32\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            for j, s in enumerate(batch):\n",
        "                results.append({\n",
        "                    \"sentence\": s,\n",
        "                    \"pos_val\": probs[j][0],\n",
        "                    \"neg_val\": probs[j][1]\n",
        "                })\n",
        "        return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. Ejecuci贸n Batch con Selector de Tickers\n",
        "\n",
        "# Inicializar Modelos\n",
        "print(\"[*] Inicializando Motor NLP...\")\n",
        "loader = SECLoader()\n",
        "model = FinBertModel()\n",
        "prep = TextPreprocessor()\n",
        "\n",
        "# --- CONFIGURACIN DE USUARIO ---\n",
        "TICKERS_INPUT = \"META, AAPL, MSFT, GOOGL, AMZN\" # @param {type:\"string\"}\n",
        "NUM_REPORTS = 5 # @param {type:\"integer\"}\n",
        "\n",
        "# Parsear tickers (separados por coma)\n",
        "TICKERS = [t.strip().upper() for t in TICKERS_INPUT.split(',') if t.strip()]\n",
        "print(f\"\\n TARGETS: {TICKERS}\")\n",
        "print(f\" REPORTES SOLICITADOS (HISTRICO): {NUM_REPORTS}\")\n",
        "\n",
        "def analyze_ticker_history(ticker_symbol, amount=1):\n",
        "    \"\"\"Funci贸n Maestra: Procesa m煤ltiples reportes hist贸ricos para un ticker.\"\"\"\n",
        "    print(f\"\\n\" + \"-\"*50)\n",
        "    print(f\"  PROCESANDO AGENTE: {ticker_symbol} (Historic)\")\n",
        "    print(\"-\"*50)\n",
        "    try:\n",
        "        # 1. Download\n",
        "        loader.download_filings(ticker_symbol, amount=amount)\n",
        "        \n",
        "        # 2. Process\n",
        "        docs = loader.process_filings(ticker_symbol)\n",
        "        if not docs:\n",
        "            print(f\"[!] No se encontraron reportes legibles para {ticker_symbol}\")\n",
        "            return []\n",
        "            \n",
        "        reports_to_scan = docs[:amount]\n",
        "        print(f\"[>] Se encontraron {len(reports_to_scan)} reportes para analizar.\")\n",
        "        \n",
        "        history_results = []\n",
        "        \n",
        "        for doc in reports_to_scan:\n",
        "            report_date = doc.get('date', 'Unknown')\n",
        "            print(f\"   --> Analizando reporte del {report_date}...\")\n",
        "            \n",
        "            # 3. Clean & Predict\n",
        "            sentences = prep.split_sentences(prep.clean_text(doc['text']))\n",
        "            if len(sentences) < 10:\n",
        "                continue # Skip low-quality docs\n",
        "                \n",
        "            df = model.predict(sentences)\n",
        "            \n",
        "            if not df.empty:\n",
        "                # 4. Calculate Metrics\n",
        "                net_score = df['pos_val'].mean() - df['neg_val'].mean()\n",
        "                history_results.append({\n",
        "                    'ticker': ticker_symbol,\n",
        "                    'date': report_date,\n",
        "                    'net_score': net_score\n",
        "                })\n",
        "        \n",
        "        return history_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Fallo cr铆tico en {ticker_symbol}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- BUCLE PRINCIPAL ---\n",
        "full_history_data = []\n",
        "\n",
        "for t in TICKERS:\n",
        "    results = analyze_ticker_history(t, amount=NUM_REPORTS)\n",
        "    full_history_data.extend(results)\n",
        "\n",
        "# Construir DataFrame Maestro\n",
        "df_history = pd.DataFrame(full_history_data)\n",
        "\n",
        "if not df_history.empty:\n",
        "    # --- DATA PREP PARA VISUALIZACIN ---\n",
        "    df_history['date_obj'] = pd.to_datetime(df_history['date'])\n",
        "    df_history['year'] = df_history['date_obj'].dt.year\n",
        "    \n",
        "    # Calcular Z-Score global (para comparabilidad)\n",
        "    global_mean = df_history['net_score'].mean()\n",
        "    global_std = df_history['net_score'].std()\n",
        "    if global_std == 0: global_std = 1\n",
        "    df_history['z_score'] = (df_history['net_score'] - global_mean) / global_std\n",
        "\n",
        "    # --- VISUALIZACIN 1: MACRO HEATMAP (Sentimiento Hist贸rico) ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" MACRO SENTIMENT HEATMAP\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    # Pivot Table: Index=Ticker, Columns=Year, Values=Z-Score\n",
        "    # Agrupamos por a帽o tomando el promedio si hay multiples reportes en un a帽o\n",
        "    pivot_table = df_history.pivot_table(index='ticker', columns='year', values='z_score', aggfunc='mean')\n",
        "    \n",
        "    plt.figure(figsize=(12, len(TICKERS)*1.2 + 2))\n",
        "    sns.heatmap(\n",
        "        pivot_table, \n",
        "        cmap='RdYlGn', \n",
        "        center=0, \n",
        "        annot=True, \n",
        "        fmt=\".2f\", \n",
        "        linewidths=.5, \n",
        "        cbar_kws={'label': 'Z-Score (Desviaci贸n del Promedio)'}\n",
        "    )\n",
        "    plt.title('Evoluci贸n Hist贸rica del Sentimiento Sectorial', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('Empresa')\n",
        "    plt.xlabel('A帽o Fiscal')\n",
        "    plt.show()\n",
        "\n",
        "    # --- VISUALIZACIN 2: ALPHA HUNTER (Solo 煤ltimo a帽o disponible) ---\n",
        "    # Filtramos para quedarnos con el registro m谩s reciente de cada ticker para el scatter plot\n",
        "    latest_indices = df_history.groupby('ticker')['date_obj'].idxmax()\n",
        "    df_sector_latest = df_history.loc[latest_indices].copy()\n",
        "    \n",
        "    print(\"\\n[*] Calculando Alpha (Market Returns) para el periodo m谩s reciente...\")\n",
        "    \n",
        "    # Descarga optimizada de precios\n",
        "    min_date = df_sector_latest['date_obj'].min() - timedelta(days=5)\n",
        "    max_date = datetime.now() + timedelta(days=1)\n",
        "    tickers_list = df_sector_latest['ticker'].tolist()\n",
        "    \n",
        "    try:\n",
        "        market_data = yf.download(tickers_list, start=min_date, end=max_date, progress=False)['Close']\n",
        "        if len(tickers_list) == 1: market_data = pd.DataFrame({tickers_list[0]: market_data})\n",
        "\n",
        "        perf_list = []\n",
        "        for idx, row in df_sector_latest.iterrows():\n",
        "            t = row['ticker']\n",
        "            d_start = row['date_obj']\n",
        "            d_end = d_start + timedelta(days=180)\n",
        "            if d_end > datetime.now(): d_end = datetime.now() - timedelta(days=1)\n",
        "            \n",
        "            if t in market_data.columns:\n",
        "                ts = market_data[t].dropna()\n",
        "                future_prices = ts[ts.index >= d_start]\n",
        "                past_prices = ts[ts.index <= d_end]\n",
        "                \n",
        "                if not future_prices.empty and not past_prices.empty:\n",
        "                    p_start = future_prices.iloc[0]\n",
        "                    p_end = past_prices.iloc[-1]\n",
        "                    perf_list.append((p_end - p_start) / p_start * 100)\n",
        "                else:\n",
        "                    perf_list.append(0.0)\n",
        "            else:\n",
        "                perf_list.append(0.0)\n",
        "                \n",
        "        df_sector_latest['price_return_6m'] = perf_list\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"[!] Error Market Data: {e}\")\n",
        "        df_sector_latest['price_return_6m'] = 0.0\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        data=df_sector_latest, x='z_score', y='price_return_6m', \n",
        "        s=150, color='#2E86AB', edgecolor='black', alpha=0.8\n",
        "    )\n",
        "    for i in range(df_sector_latest.shape[0]):\n",
        "        plt.text(\n",
        "            x=df_sector_latest.z_score.iloc[i]+0.02, \n",
        "            y=df_sector_latest.price_return_6m.iloc[i]+0.5, \n",
        "            s=df_sector_latest.ticker.iloc[i], \n",
        "            fontweight='bold', fontsize=10\n",
        "        )\n",
        "    plt.axvline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.title('Alpha Hunter: IA Sentiment vs 6-Month Post-Filing Return (Latest Report)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Sentiment Z-Score (IA)', fontsize=12)\n",
        "    plt.ylabel('Post-Filing Price Return (%)', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[!] No data for visualization.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}