{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 游 FinSentiment 3.0: Sector Screener (Batch Edition)\n",
        "\n",
        "Versi칩n escalable para analizar m칰ltiples empresas del sector tecnol칩gico de forma secuencial y comparativa.\n",
        "\n",
        "### Nuevas Caracter칤sticas (Fase 7 - 9):\n",
        "1. **Quarterly Pulse**: An치lisis de alta frecuencia (10-K + 10-Q).\n",
        "2. **Dynamic Z-Score**: Normalizaci칩n adaptativa (Rolling Window 2 a침os).\n",
        "3. **Alpha Hunter**: Scatter Plot Sincronizado (IA vs Market).\n",
        "4. **Timeline**: Visualizaci칩n de la evoluci칩n trimestral del sentimiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. Instalaci칩n de Dependencias\n",
        "!pip install sec-edgar-downloader transformers torch pandas numpy matplotlib seaborn beautifulsoup4 yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. Definici칩n del Motor (Core Engine v2.1 + Quarterly Extension)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Configurar estilos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "\n",
        "# --- M칍DULO 1: INGESTA (Quarterly Upgrade) ---\n",
        "class SECLoader:\n",
        "    def __init__(self, data_dir=\"data\", email=\"research@example.com\", company=\"Personal Research\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
        "        self.downloader = Downloader(company, email, os.path.join(data_dir, \"raw\"))\n",
        "\n",
        "    def download_filings(self, ticker: str, amount: int = 4):\n",
        "        # Convertimos amount anual a aproximado trimestral si es bajo, \n",
        "        # pero aqui el usuario suele pedir 'N reportes hacia atras'.\n",
        "        print(f\"[>] ({ticker}) Descargando 칰ltimos {amount} reportes (10-K y 10-Q)...\")\n",
        "        try:\n",
        "            # Descargamos ambos tipos. 'amount' se aplica a cada uno por seguridad para tener cobertura.\n",
        "            self.downloader.get(\"10-K\", ticker, limit=amount)\n",
        "            self.downloader.get(\"10-Q\", ticker, limit=amount)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error en descarga {ticker}: {e}\")\n",
        "\n",
        "    def extract_date(self, content: str) -> str:\n",
        "        patterns = [\n",
        "            r'FILED AS OF DATE:\\s+(\\d{8})',\n",
        "            r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})'\n",
        "        ]\n",
        "        for p in patterns:\n",
        "            match = re.search(p, content)\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "                return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
        "        return None\n",
        "\n",
        "    def extract_mda(self, html_content: str) -> str:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        # MD&A es Item 7 en 10-K, pero Item 2 en 10-Q\n",
        "        patterns = [\n",
        "            r'Item\\s+7\\.\\s+Management', \n",
        "            r'Item\\s+2\\.\\s+Management',\n",
        "            r\"Management's\\s+Discussion\", \n",
        "        ]\n",
        "        start_idx = -1\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text, re.IGNORECASE)\n",
        "            if match: start_idx = match.start(); break\n",
        "        if start_idx == -1: return text[:50000]\n",
        "        return text[start_idx:start_idx+30000]\n",
        "\n",
        "    def process_filings(self, ticker: str):\n",
        "        # Explorar carpetas 10-K y 10-Q por separado\n",
        "        base_path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        processed_data = []\n",
        "        \n",
        "        for report_type in [\"10-K\", \"10-Q\"]:\n",
        "            type_path = os.path.join(base_path, report_type)\n",
        "            if not os.path.exists(type_path): continue\n",
        "            \n",
        "            for root, _, files in os.walk(type_path):\n",
        "                for file in files:\n",
        "                    if file.lower().endswith(\".txt\") and \"primary\" not in file:\n",
        "                        try:\n",
        "                            with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                                content = f.read()\n",
        "                            mda = self.extract_mda(content)\n",
        "                            date = self.extract_date(content)\n",
        "                            if len(mda) > 500:\n",
        "                                processed_data.append({\n",
        "                                    'text': mda, \n",
        "                                    'date': date, \n",
        "                                    'type': report_type,\n",
        "                                    'accession': file\n",
        "                                })\n",
        "                        except: pass\n",
        "                        \n",
        "        # Ordenar por fecha cronol칩gica inversa (m치s reciente primero)\n",
        "        return sorted(processed_data, key=lambda x: x.get('date', '1900'), reverse=True)\n",
        "\n",
        "# --- M칍DULO 2 NOISE FILTER ---\n",
        "class TextPreprocessor:\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return \"\".join(ch for ch in text if ch.isprintable()).strip()\n",
        "    \n",
        "    def is_legal_noise(self, sentence: str) -> bool:\n",
        "        legal_keywords = [\n",
        "            'forward-looking', 'safe harbor', 'uncertainty', 'may differ', \n",
        "            'subject to error', 'actual results', 'factors that could cause',\n",
        "            'statements regarding', 'cautionary note', 'risk factors', \n",
        "            'include but are not limited to', 'assumptions'\n",
        "        ]\n",
        "        return any(kw in sentence.lower() for kw in legal_keywords)\n",
        "\n",
        "    def split_sentences(self, text):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "        valid_sentences = []\n",
        "        for s in sentences:\n",
        "            s = s.strip()\n",
        "            if len(s) > 20 and len(s.split()) >= 4:\n",
        "                if not self.is_legal_noise(s):\n",
        "                    valid_sentences.append(s)\n",
        "        return valid_sentences\n",
        "\n",
        "# --- M칍DULO 3 FINBERT ---\n",
        "class FinBertModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"[*] Cargando FinBERT en {self.device}...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(self.device)\n",
        "        self.labels = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        if not sentences: return pd.DataFrame()\n",
        "        batch_size = 32\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            for j, s in enumerate(batch):\n",
        "                results.append({\n",
        "                    \"sentence\": s,\n",
        "                    \"pos_val\": probs[j][0],\n",
        "                    \"neg_val\": probs[j][1]\n",
        "                })\n",
        "        return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. Ejecuci칩n Batch de Alta Frecuencia (Quarterly)\n",
        "\n",
        "# Inicializar Modelos\n",
        "print(\"[*] Inicializando Motor NLP...\")\n",
        "loader = SECLoader()\n",
        "model = FinBertModel()\n",
        "prep = TextPreprocessor()\n",
        "\n",
        "# --- CONFIGURACI칍N DE USUARIO ---\n",
        "TICKERS_INPUT = \"META, AAPL, MSFT, GOOGL, AMZN\" # @param {type:\"string\"}\n",
        "NUM_REPORTS = 8 # @param {type:\"integer\"} \n",
        "# NOTA: Con 8 reportes cubrimos aprox 2 a침os de historia (4Q por a침o)\n",
        "\n",
        "# Parsear tickers \n",
        "TICKERS = [t.strip().upper() for t in TICKERS_INPUT.split(',') if t.strip()]\n",
        "print(f\"\\n游닇 TARGETS: {TICKERS}\")\n",
        "print(f\"游늼 VENTANA DE AN츼LISIS: {NUM_REPORTS} Reportes (10-K/10-Q Mixtos)\")\n",
        "\n",
        "def analyze_quarterly_pulse(ticker_symbol, amount=4):\n",
        "    \"\"\"Procesa flujo mixto trimestral para obtener granularidad.\"\"\"\n",
        "    print(f\"\\n\" + \"-\"*50)\n",
        "    print(f\" 游끽 PROCESANDO AGENTE: {ticker_symbol} (Quarterly Mode)\")\n",
        "    print(\"-\"*50)\n",
        "    try:\n",
        "        # 1. Download (Combined K & Q)\n",
        "        loader.download_filings(ticker_symbol, amount=amount)\n",
        "        \n",
        "        # 2. Process\n",
        "        docs = loader.process_filings(ticker_symbol)\n",
        "        if not docs:\n",
        "            print(f\"[!] No data found for {ticker_symbol}\")\n",
        "            return []\n",
        "            \n",
        "        # Tomamos los 'amount' m치s recientes (ya est치n ordenados)\n",
        "        reports_to_scan = docs[:amount]\n",
        "        # Re-ordenamos cronol칩gicamente ASCENDENTE para el c치lculo de Rolling Window l칩gico\n",
        "        reports_to_scan = sorted(reports_to_scan, key=lambda x: x.get('date', '1900'))\n",
        "\n",
        "        print(f\"[>] Analizando serie temporal de {len(reports_to_scan)} puntos...\")\n",
        "        \n",
        "        history_results = []\n",
        "        \n",
        "        for doc in reports_to_scan:\n",
        "            report_date = doc.get('date', 'Unknown')\n",
        "            r_type = doc.get('type', 'UNK')\n",
        "            print(f\"   --> [{r_type}] {report_date}\")\n",
        "            \n",
        "            # 3. Clean & Predict\n",
        "            sentences = prep.split_sentences(prep.clean_text(doc['text']))\n",
        "            if len(sentences) < 5: continue\n",
        "                \n",
        "            df = model.predict(sentences)\n",
        "            \n",
        "            if not df.empty:\n",
        "                net_score = df['pos_val'].mean() - df['neg_val'].mean()\n",
        "                history_results.append({\n",
        "                    'ticker': ticker_symbol,\n",
        "                    'date': report_date,\n",
        "                    'type': r_type,\n",
        "                    'net_score': net_score\n",
        "                })\n",
        "        \n",
        "        return history_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {ticker_symbol}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- BUCLE PRINCIPAL ---\n",
        "full_history_data = []\n",
        "\n",
        "for t in TICKERS:\n",
        "    results = analyze_quarterly_pulse(t, amount=NUM_REPORTS)\n",
        "    full_history_data.extend(results)\n",
        "\n",
        "# Construir DataFrame\n",
        "df_history = pd.DataFrame(full_history_data)\n",
        "\n",
        "if not df_history.empty:\n",
        "    # --- DATA PREP: DYNAMIC ROLLING Z-SCORE ---\n",
        "    df_history['date_obj'] = pd.to_datetime(df_history['date'])\n",
        "    \n",
        "    print(\"\\n[*] Aplicando Rolling Z-Score (Ventana Adaptativa)... \")\n",
        "    \n",
        "    # Funci칩n para Rolling Z-Score por grupo\n",
        "    def calculate_rolling_z(group):\n",
        "        # Ventana de 8 periodos (aprox 2 a침os de trimestres)\n",
        "        # min_periods=2 para que empiece a calcular pronto\n",
        "        roller = group.rolling(window=8, min_periods=2)\n",
        "        mean = roller.mean()\n",
        "        std = roller.std().replace(0, 1) # Evitar div/0\n",
        "        return (group - mean) / std\n",
        "\n",
        "    df_history = df_history.sort_values(['ticker', 'date_obj'])\n",
        "    df_history['z_score_dynamic'] = df_history.groupby('ticker')['net_score'].transform(calculate_rolling_z)\n",
        "    \n",
        "    # Rellenar NaNs iniciales con 0 o Z-Score est치tico si se prefiere\n",
        "    df_history['z_score_dynamic'] = df_history['z_score_dynamic'].fillna(0)\n",
        "\n",
        "    # --- VISUALIZACI칍N 1: TIMELINE (Gr치fico de L칤nea Solicitado) ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" SENTIMENT TICKER TIMELINE\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.lineplot(\n",
        "        data=df_history, \n",
        "        x='date_obj', \n",
        "        y='z_score_dynamic', \n",
        "        hue='ticker', \n",
        "        marker='o',\n",
        "        palette='tab10',\n",
        "        linewidth=2.5\n",
        "    )\n",
        "    \n",
        "    plt.title('Evoluci칩n Trimestral del Sentimiento (Dynamic Z-Score)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Fecha de Reporte')\n",
        "    plt.ylabel('Dynamic Z-Score (vs Historia Reciente)')\n",
        "    plt.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- VISUALIZACI칍N 2: ALPHA HUNTER (Ultimo punto) ---\n",
        "    # Usamos el 칰ltimo dato de la serie din치mica\n",
        "    latest_indices = df_history.groupby('ticker')['date_obj'].idxmax()\n",
        "    df_latest = df_history.loc[latest_indices].copy()\n",
        "    \n",
        "    # Calcular Retorno Post-Filing para estos 칰ltimos puntos\n",
        "    print(\"\\n[*] Actualizando Alpha Hunter (Quarterly Precision)...\")\n",
        "    min_date = df_latest['date_obj'].min() - timedelta(days=5)\n",
        "    max_date = datetime.now() + timedelta(days=1)\n",
        "    tickers_list = df_latest['ticker'].tolist()\n",
        "    \n",
        "    try:\n",
        "        market_data = yf.download(tickers_list, start=min_date, end=max_date, progress=False)['Close']\n",
        "        if len(tickers_list) == 1: market_data = pd.DataFrame({tickers_list[0]: market_data})\n",
        "\n",
        "        perf_list = []\n",
        "        for idx, row in df_latest.iterrows():\n",
        "            t = row['ticker']\n",
        "            d_start = row['date_obj']\n",
        "            # Ventana m치s corta para trimestral: 3 Meses\n",
        "            d_end = d_start + timedelta(days=90) \n",
        "            if d_end > datetime.now(): d_end = datetime.now() - timedelta(days=1)\n",
        "            \n",
        "            if t in market_data.columns:\n",
        "                ts = market_data[t].dropna()\n",
        "                future_prices = ts[ts.index >= d_start]\n",
        "                past_prices = ts[ts.index <= d_end]\n",
        "                \n",
        "                if not future_prices.empty and not past_prices.empty:\n",
        "                    p_start = future_prices.iloc[0]\n",
        "                    p_end = past_prices.iloc[-1]\n",
        "                    perf_list.append((p_end - p_start) / p_start * 100)\n",
        "                else:\n",
        "                    perf_list.append(0.0)\n",
        "            else:\n",
        "                perf_list.append(0.0)\n",
        "        df_latest['price_return_3m'] = perf_list\n",
        "    except: \n",
        "        df_latest['price_return_3m'] = 0.0\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        data=df_latest, x='z_score_dynamic', y='price_return_3m', \n",
        "        s=200, color='#8e44ad', edgecolor='black', alpha=0.8\n",
        "    )\n",
        "    for i in range(df_latest.shape[0]):\n",
        "        plt.text(\n",
        "            x=df_latest.z_score_dynamic.iloc[i]+0.02, \n",
        "            y=df_latest.price_return_3m.iloc[i]+0.2, \n",
        "            s=df_latest.ticker.iloc[i], \n",
        "            fontweight='bold'\n",
        "        )\n",
        "    plt.axvline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.title('Alpha Hunter QoQ: Sentiment vs 3-Month Return', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Dynamic Z-Score (vs 2-Year History)', fontsize=12)\n",
        "    plt.ylabel('3-Month Price Return (%)', fontsize=12)\n",
        "    plt.show()\n",
        "    \n",
        "    # Mostrar Tabla Resumen\n",
        "    display(df_history[['ticker', 'date', 'type', 'net_score', 'z_score_dynamic']].sort_values(['ticker', 'date'], ascending=False).head(10))\n",
        "else:\n",
        "    print(\"[!] No data analyzed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}