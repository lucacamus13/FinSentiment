{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  FinSentiment 3.0: Sector Screener (Quarterly Edition)\n",
        "\n",
        "Versi贸n avanzada para an谩lisis de alta frecuencia (Trimestral) con normalizaci贸n estratificada.\n",
        "\n",
        "### Nuevas Caracter铆sticas (Fase 9):\n",
        "1. **Quarterly Pulse**: Ingesta h铆brida 10-K (Anual) + 10-Q (Trimestral).\n",
        "2. **Stratified Z-Score**: Normalizaci贸n por tipo de documento (corrige el sesgo negativo de los 10-K).\n",
        "3. **Timeline & Heatmap**: Visualizaci贸n de la evoluci贸n temporal ajustada.\n",
        "4. **Alpha Hunter QoQ**: Correlaci贸n con retornos trimestrales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 1. Instalaci贸n de Dependencias\n",
        "!pip install sec-edgar-downloader transformers torch pandas numpy matplotlib seaborn beautifulsoup4 yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 2. Definici贸n del Motor (Core Engine v2.1 + Quarterly Extension)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from collections import Counter\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from sec_edgar_downloader import Downloader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Configurar estilos\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [14, 7]\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# --- MDULO 1: INGESTA (Quarterly Upgrade) ---\n",
        "class SECLoader:\n",
        "    def __init__(self, data_dir=\"data\", email=\"research@example.com\", company=\"Personal Research\"):\n",
        "        self.data_dir = data_dir\n",
        "        os.makedirs(os.path.join(data_dir, \"raw\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
        "        self.downloader = Downloader(company, email, os.path.join(data_dir, \"raw\"))\n",
        "\n",
        "    def download_filings(self, ticker: str, amount: int = 4):\n",
        "        print(f\"[>] ({ticker}) Descargando 煤ltimos reportes (Target: {amount}x 10-K, {amount}x 10-Q)...\")\n",
        "        try:\n",
        "            self.downloader.get(\"10-K\", ticker, limit=amount)\n",
        "            self.downloader.get(\"10-Q\", ticker, limit=amount)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Error en descarga {ticker}: {e}\")\n",
        "\n",
        "    def extract_date(self, content: str) -> str:\n",
        "        patterns = [\n",
        "            r'FILED AS OF DATE:\\s+(\\d{8})',\n",
        "            r'CONFORMED PERIOD OF REPORT:\\s+(\\d{8})'\n",
        "        ]\n",
        "        for p in patterns:\n",
        "            match = re.search(p, content)\n",
        "            if match:\n",
        "                date_str = match.group(1)\n",
        "                return f\"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}\"\n",
        "        return None\n",
        "\n",
        "    def extract_mda(self, html_content: str) -> str:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        text = soup.get_text(separator='\\n')\n",
        "        patterns = [\n",
        "            r'Item\\s+7\\.\\s+Management', \n",
        "            r'Item\\s+2\\.\\s+Management',\n",
        "            r\"Management's\\s+Discussion\", \n",
        "        ]\n",
        "        start_idx = -1\n",
        "        for p in patterns:\n",
        "            match = re.search(p, text, re.IGNORECASE)\n",
        "            if match: start_idx = match.start(); break\n",
        "        if start_idx == -1: return text[:50000]\n",
        "        return text[start_idx:start_idx+30000]\n",
        "\n",
        "    def process_filings(self, ticker: str):\n",
        "        base_path = os.path.join(self.data_dir, \"raw\", \"sec-edgar-filings\", ticker)\n",
        "        processed_data = []\n",
        "        \n",
        "        for report_type in [\"10-K\", \"10-Q\"]:\n",
        "            type_path = os.path.join(base_path, report_type)\n",
        "            if not os.path.exists(type_path): continue\n",
        "            \n",
        "            for root, _, files in os.walk(type_path):\n",
        "                for file in files:\n",
        "                    if file.lower().endswith(\".txt\") and \"primary\" not in file:\n",
        "                        try:\n",
        "                            with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                                content = f.read()\n",
        "                            mda = self.extract_mda(content)\n",
        "                            date = self.extract_date(content)\n",
        "                            if len(mda) > 500:\n",
        "                                processed_data.append({\n",
        "                                    'text': mda, \n",
        "                                    'date': date, \n",
        "                                    'type': report_type,\n",
        "                                    'accession': file\n",
        "                                })\n",
        "                        except: pass\n",
        "        \n",
        "        return sorted(processed_data, key=lambda x: x.get('date', '1900'), reverse=True)\n",
        "\n",
        "# --- MDULO 2 NOISE FILTER ---\n",
        "class TextPreprocessor:\n",
        "    def clean_text(self, text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return \"\".join(ch for ch in text if ch.isprintable()).strip()\n",
        "    \n",
        "    def is_legal_noise(self, sentence: str) -> bool:\n",
        "        legal_keywords = [\n",
        "            'forward-looking', 'safe harbor', 'uncertainty', 'may differ', \n",
        "            'subject to error', 'actual results', 'factors that could cause',\n",
        "            'statements regarding', 'cautionary note', 'risk factors', \n",
        "            'include but are not limited to', 'assumptions'\n",
        "        ]\n",
        "        return any(kw in sentence.lower() for kw in legal_keywords)\n",
        "\n",
        "    def split_sentences(self, text):\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "        valid_sentences = []\n",
        "        for s in sentences:\n",
        "            s = s.strip()\n",
        "            if len(s) > 20 and len(s.split()) >= 4:\n",
        "                if not self.is_legal_noise(s):\n",
        "                    valid_sentences.append(s)\n",
        "        return valid_sentences\n",
        "\n",
        "# --- MDULO 3 FINBERT ---\n",
        "class FinBertModel:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"[*] Cargando FinBERT en {self.device}...\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\").to(self.device)\n",
        "        self.labels = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "\n",
        "    def predict(self, sentences):\n",
        "        if not sentences: return pd.DataFrame()\n",
        "        batch_size = 32\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            for j, s in enumerate(batch):\n",
        "                results.append({\n",
        "                    \"sentence\": s,\n",
        "                    \"pos_val\": probs[j][0],\n",
        "                    \"neg_val\": probs[j][1]\n",
        "                })\n",
        "        return pd.DataFrame(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title 3. Ejecuci贸n Batch de Alta Frecuencia (Quarterly)\n",
        "\n",
        "# Inicializar Modelos\n",
        "print(\"[*] Inicializando Motor NLP...\")\n",
        "loader = SECLoader()\n",
        "model = FinBertModel()\n",
        "prep = TextPreprocessor()\n",
        "\n",
        "# --- CONFIGURACIN DE USUARIO ---\n",
        "TICKERS_INPUT = \"META, AAPL, MSFT, GOOGL, AMZN\" # @param {type:\"string\"}\n",
        "NUM_REPORTS = 8 # @param {type:\"integer\"} \n",
        "\n",
        "# Parsear tickers\n",
        "TICKERS = [t.strip().upper() for t in TICKERS_INPUT.split(',') if t.strip()]\n",
        "print(f\"\\n TARGETS: {TICKERS}\")\n",
        "print(f\" VENTANA DE ANLISIS: {NUM_REPORTS} Reportes (Mix 10-K/10-Q)\")\n",
        "\n",
        "def analyze_quarterly_pulse(ticker_symbol, amount=4):\n",
        "    print(f\"\\n\" + \"-\"*50)\n",
        "    print(f\"  PROCESANDO AGENTE: {ticker_symbol} (Quarterly Mode)\")\n",
        "    print(\"-\"*50)\n",
        "    try:\n",
        "        loader.download_filings(ticker_symbol, amount=amount)\n",
        "        docs = loader.process_filings(ticker_symbol)\n",
        "        if not docs:\n",
        "            print(f\"[!] No data found for {ticker_symbol}\")\n",
        "            return []\n",
        "            \n",
        "        reports_to_scan = docs[:amount]\n",
        "        # Ordenar CRONOLGICAMENTE para los gr谩ficos de linea\n",
        "        reports_to_scan = sorted(reports_to_scan, key=lambda x: x.get('date', '1900'))\n",
        "\n",
        "        print(f\"[>] Analizando serie temporal de {len(reports_to_scan)} puntos...\")\n",
        "        \n",
        "        history_results = []\n",
        "        \n",
        "        for doc in reports_to_scan:\n",
        "            report_date = doc.get('date', 'Unknown')\n",
        "            r_type = doc.get('type', 'UNK')\n",
        "            print(f\"   --> [{r_type}] {report_date}\")\n",
        "            \n",
        "            sentences = prep.split_sentences(prep.clean_text(doc['text']))\n",
        "            if len(sentences) < 5: continue\n",
        "                \n",
        "            df = model.predict(sentences)\n",
        "            \n",
        "            if not df.empty:\n",
        "                net_score = df['pos_val'].mean() - df['neg_val'].mean()\n",
        "                history_results.append({\n",
        "                    'ticker': ticker_symbol,\n",
        "                    'date': report_date,\n",
        "                    'type': r_type,\n",
        "                    'net_score': net_score\n",
        "                })\n",
        "        \n",
        "        return history_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {ticker_symbol}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- BUCLE PRINCIPAL ---\n",
        "full_history_data = []\n",
        "\n",
        "for t in TICKERS:\n",
        "    results = analyze_quarterly_pulse(t, amount=NUM_REPORTS)\n",
        "    full_history_data.extend(results)\n",
        "\n",
        "# Construir DataFrame\n",
        "df_history = pd.DataFrame(full_history_data)\n",
        "\n",
        "if not df_history.empty:\n",
        "    df_history['date_obj'] = pd.to_datetime(df_history['date'])\n",
        "    \n",
        "    # --- STRATIFIED NORMALIZATION (New Logic) ---\n",
        "    print(\"\\n[*] Aplicando Stratified Normalization (Tipo de Reporte)... \")\n",
        "    \n",
        "    # 1. Agrupar por 'type' (10-K vs 10-Q) para calcular estad铆sticas sectoriales\n",
        "    # Esto nos da la \"Baseline\" de cada tipo de documento.\n",
        "    stats_by_type = df_history.groupby('type')['net_score'].agg(['mean', 'std']).reset_index()\n",
        "    print(\"\\nStats por Tipo de Reporte (Baseline):\")\n",
        "    display(stats_by_type)\n",
        "    \n",
        "    # 2. Aplicar Z-Score usando la media/std correspondiente a su tipo\n",
        "    def apply_stratified_z(row):\n",
        "        stats = stats_by_type[stats_by_type['type'] == row['type']].iloc[0]\n",
        "        mu = stats['mean']\n",
        "        sigma = stats['std']\n",
        "        if sigma == 0: sigma = 1\n",
        "        return (row['net_score'] - mu) / sigma\n",
        "\n",
        "    df_history['z_score_stratified'] = df_history.apply(apply_stratified_z, axis=1)\n",
        "    \n",
        "    # 3. Ordenar para visualizaci贸n\n",
        "    df_history = df_history.sort_values(['ticker', 'date_obj'])\n",
        "\n",
        "    # --- VISUALIZACIN 1: TIMELINE ESTRATIFICADO ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" STRATIFIED SENTIMENT TIMELINE\")\n",
        "    print(\"=\"*40)\n",
        "    \n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.lineplot(\n",
        "        data=df_history, \n",
        "        x='date_obj', \n",
        "        y='z_score_stratified', \n",
        "        hue='ticker', \n",
        "        marker='o',\n",
        "        palette='tab10',\n",
        "        linewidth=2.5\n",
        "    )\n",
        "    \n",
        "    plt.title('Evoluci贸n Trimestral Estabilizada (Z-Score Ajustado por Tipo)', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Fecha de Reporte')\n",
        "    plt.ylabel('Stratified Z-Score (Desviaci贸n T铆pica vs Baseline del Tipo)')\n",
        "    plt.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- VISUALIZACIN 2: ALPHA HUNTER (Ultimo punto) ---\n",
        "    latest_indices = df_history.groupby('ticker')['date_obj'].idxmax()\n",
        "    df_latest = df_history.loc[latest_indices].copy()\n",
        "    \n",
        "    print(\"\\n[*] Actualizando Alpha Hunter (Quarterly Precision)...\")\n",
        "    min_date = df_latest['date_obj'].min() - timedelta(days=5)\n",
        "    max_date = datetime.now() + timedelta(days=1)\n",
        "    tickers_list = df_latest['ticker'].tolist()\n",
        "    \n",
        "    try:\n",
        "        market_data = yf.download(tickers_list, start=min_date, end=max_date, progress=False)['Close']\n",
        "        if len(tickers_list) == 1: market_data = pd.DataFrame({tickers_list[0]: market_data})\n",
        "\n",
        "        perf_list = []\n",
        "        for idx, row in df_latest.iterrows():\n",
        "            t = row['ticker']\n",
        "            d_start = row['date_obj']\n",
        "            d_end = d_start + timedelta(days=90) \n",
        "            if d_end > datetime.now(): d_end = datetime.now() - timedelta(days=1)\n",
        "            \n",
        "            if t in market_data.columns:\n",
        "                ts = market_data[t].dropna()\n",
        "                future_prices = ts[ts.index >= d_start]\n",
        "                past_prices = ts[ts.index <= d_end]\n",
        "                \n",
        "                if not future_prices.empty and not past_prices.empty:\n",
        "                    p_start = future_prices.iloc[0]\n",
        "                    p_end = past_prices.iloc[-1]\n",
        "                    perf_list.append((p_end - p_start) / p_start * 100)\n",
        "                else:\n",
        "                    perf_list.append(0.0)\n",
        "            else:\n",
        "                perf_list.append(0.0)\n",
        "        df_latest['price_return_3m'] = perf_list\n",
        "    except: \n",
        "        df_latest['price_return_3m'] = 0.0\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(\n",
        "        data=df_latest, x='z_score_stratified', y='price_return_3m', \n",
        "        s=200, hue='ticker', palette='tab10', edgecolor='black', alpha=0.9\n",
        "    )\n",
        "    for i in range(df_latest.shape[0]):\n",
        "        plt.text(\n",
        "            x=df_latest.z_score_stratified.iloc[i]+0.02, \n",
        "            y=df_latest.price_return_3m.iloc[i]+0.2, \n",
        "            s=df_latest.ticker.iloc[i], \n",
        "            fontweight='bold', fontsize=9\n",
        "        )\n",
        "    plt.axvline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.axhline(0, color='gray', linestyle='--', alpha=0.6)\n",
        "    plt.title('Alpha Hunter QoQ (Stratified): Sentiment vs 3-Month Return', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Stratified Z-Score (Adjusted by Report Type)', fontsize=12)\n",
        "    plt.ylabel('3-Month Price Return (%)', fontsize=12)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"[!] No data analyzed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}